{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b094e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the following confusion matrix, evaluate (by hand) \n",
    "# the model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79752c85",
   "metadata": {},
   "source": [
    "In the context of this problem, what is a false positive?\n",
    "\n",
    "<br>\n",
    "\n",
    "we predict the cat and it is actually a dog\n",
    "the 7 number\n",
    "\n",
    "<br>\n",
    "In the context of this problem, what is a false negative?\n",
    "if we predict a dot and it is actually a cat\n",
    "\n",
    "\n",
    "<br>\n",
    "How would you describe this model?\n",
    "<br>\n",
    "backwards from the origional way we were taught made more confusing for no reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32b90c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5625\n",
      "0.8125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>model2</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>81</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model2     Defect  No Defect\n",
       "actual                      \n",
       "Defect          9          7\n",
       "No Defect      81        103"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# You are working as a datascientist working for Codeup Cody \n",
    "# Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n",
    "df = pd.read_csv('c3.csv')\n",
    "\n",
    "df\n",
    "# Unfortunately, some of the rubber ducks that are produced \n",
    "# will have defects. Your team has built several models that \n",
    "# try to predict those defects, and the data from their predictions \n",
    "# can be found here.\n",
    "\n",
    "\n",
    "\n",
    "# Use the predictions dataset and pandas to help answer the \n",
    "# following questions\n",
    "# An internal team wants to investigate the cause of the \n",
    "# manufacturing defects. \n",
    "\n",
    "\n",
    "\n",
    "##usefull info\n",
    "# They tell you that they want to identify as many of the ducks \n",
    "# that have a defect as possible.\n",
    "##usefull info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Which evaluation metric would be appropriate here? \n",
    "# sensitivity/ recall\n",
    "\n",
    "\n",
    "# Which model would be the best fit for this use case?\n",
    "# TP/(TP+FN)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_1_df = df[df.actual == 'Defect']\n",
    "\n",
    "\n",
    "# TP_model_1 = model_1_df[model_1_df.actual == model_1_df.model1].shape[0]\n",
    "# FN_model_1 = model_1_df[model_1_df.actual != model_1_df.model1].shape[0]\n",
    "\n",
    "\n",
    "# sensitivity_model_1 = TP_model_1 / (TP_model_1 + FN_model_1) * 100  ##sensitivity model 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model_2_df = df[df.actual == 'Defect']\n",
    "# TP_model_2 = model_2_df[model_2_df.actual == model_2_df.model1].shape[0]\n",
    "# FN_model_2 = model_2_df[model_2_df.actual != model_2_df.model1].shape[0]\n",
    "\n",
    "\n",
    "# sensitivity_model_2 = TP_model_2 / (TP_model_2 + FN_model_2) * 100 \n",
    "\n",
    "\n",
    "# model_3_df = df[df.actual == 'Defect']\n",
    "# TP_model_3 = model_3_df[model_3_df.actual == model_3_df.model1].shape[0]\n",
    "# FN_model_3 = model_3_df[model_3_df.actual != model_3_df.model1].shape[0]\n",
    "\n",
    "\n",
    "# sensitivity_model_3 = TP_model_3 / (TP_model_3 + FN_model_3) * 100 \n",
    "\n",
    "\n",
    "# sensitivity_model_3\n",
    "# pd.crosstab(df.actual, df.model1)\n",
    "\n",
    "\n",
    "# FN = predicted it to be negative but it was actually true\n",
    "\n",
    "df['baseline_prediction'] = \"Defect\"\n",
    "\n",
    "# model_accuracy = (df.actual == df.model1).mean() * 100\n",
    "\n",
    "# baseline_accuracy = (df.actual == df.baseline_prediction).mean()\n",
    "\n",
    "\n",
    "\n",
    "subset = df[df.actual == 'Defect']\n",
    "\n",
    "\n",
    "print((subset.actual == subset.model1).mean())\n",
    "print((subset.actual == subset.model2).mean())\n",
    "print((subset.actual == subset.model3).mean())\n",
    "pd.crosstab(df.actual, df.model2)\n",
    "\n",
    "# precision/PPV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ec3dccd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent correct Defects 4.21\n",
      "percent correct Defects 8.04\n",
      "percent correct Defects 11.71\n",
      "percent correct no Defects 95.79\n",
      "percent correct no Defects 91.96\n",
      "percent correct no Defects 88.29\n"
     ]
    }
   ],
   "source": [
    "model_1_df = df[df.actual == 'Defect']\n",
    "model_accuracy = (df.actual == df.model1)\n",
    "model_accuracy = df[model_accuracy]\n",
    "model1_accuracy = (model_accuracy.model1 == 'Defect').mean()\n",
    "print('percent correct Defects',(model1_accuracy * 100).round(2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_2_df = df[df.actual == 'Defect']\n",
    "model_accuracy = (df.actual == df.model2)\n",
    "model_accuracy = df[model_accuracy]\n",
    "model2_accuracy = (model_accuracy.model2 == 'Defect').mean()\n",
    "print('percent correct Defects',(model2_accuracy * 100).round(2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_3_df = df[df.actual == 'Defect']\n",
    "model_accuracy = (df.actual == df.model3)\n",
    "model_accuracy = df[model_accuracy]\n",
    "model3_accuracy = (model_accuracy.model3 == 'Defect').mean()\n",
    "print(f'percent correct Defects', (model3_accuracy * 100).round(2))\n",
    "\n",
    "\n",
    "pd.crosstab(df.actual, df.model1 )\n",
    "\n",
    "\n",
    "\n",
    "model_1_df = df[df.actual == 'No Defect']\n",
    "model_accuracy = (df.actual == df.model1)\n",
    "model_accuracy = df[model_accuracy]\n",
    "model1_accuracy = (model_accuracy.model1 == 'No Defect').mean()\n",
    "print('percent correct no Defects',(model1_accuracy * 100).round(2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_2_df = df[df.actual == 'No Defect']\n",
    "model_accuracy = (df.actual == df.model2)\n",
    "model_accuracy = df[model_accuracy]\n",
    "model2_accuracy = (model_accuracy.model2 == 'No Defect').mean()\n",
    "print('percent correct no Defects',(model2_accuracy * 100).round(2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_3_df = df[df.actual == 'No Defect']\n",
    "model_accuracy = (df.actual == df.model3)\n",
    "model_accuracy = df[model_accuracy]\n",
    "model3_accuracy = (model_accuracy.model3 == 'No Defect').mean()\n",
    "print(f'percent correct no Defects', (model3_accuracy * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb815e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n",
      "0.1\n",
      "0.13131313131313133\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# gives customers with a defective duck a vacation to Hawaii. \n",
    "\n",
    "\n",
    "# df['baseline_prediction'] = \"Defect\"\n",
    "\n",
    "# model_accuracy = (df.actual == df.model1).mean() * 100\n",
    "\n",
    "# baseline_accuracy = (df.actual == df.baseline_prediction).mean()\n",
    "\n",
    "\n",
    "\n",
    "# subset = df[df.actual == 'Defect']\n",
    "\n",
    "\n",
    "# TP_1 = (subset.actual == subset.model1).mean()\n",
    "# TP_2 = (subset.actual == subset.model2).mean()\n",
    "# TP_3 = (subset.actual == subset.model3).mean()\n",
    "\n",
    "# FN_1 = (subset.actual != subset.model1).mean()\n",
    "# FN_2 = (subset.actual != subset.model2).mean()\n",
    "# FN_3 = (subset.actual != subset.model3).mean()\n",
    "\n",
    "# print(TP_1/(TP_1+FN_1))\n",
    "# print(TP_2/(TP_2+FN_2))\n",
    "# print(TP_3/(TP_3+FN_3))\n",
    "\n",
    "\n",
    "# pd.crosstab(df.actual, df.model2)\n",
    "# df\n",
    "# subset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Recently several stories in the local news have come out highlighting \n",
    "# customers who received a rubber duck with a defect, and portraying C3 in a bad light. \n",
    "# The PR team has decided to launch a program that\n",
    "\n",
    "\n",
    "\n",
    "# (-) duck does not have a defect \n",
    "# (+) duck has a defect\n",
    "\n",
    "\n",
    "# They need you to predict which ducks will have defects, \n",
    "\n",
    "\n",
    "subset1 = df[df.model1 == 'Defect']\n",
    "subset2 = df[df.model2 == 'Defect']\n",
    "subset3 = df[df.model3 == 'Defect']\n",
    "\n",
    "true_positive_percentage1 = (subset1.actual == subset1.model1).mean()\n",
    "true_positive_percentage2 = (subset2.actual == subset2.model2).mean()\n",
    "true_positive_percentage3 = (subset3.actual == subset3.model3).mean()\n",
    "# but tell you the really don't want to accidentally give out a \n",
    "\n",
    "\n",
    "\n",
    "# vacation package when the duck really doesn't have a defect. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Which evaluation metric would be appropriate here? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Which model would be the best fit for this use case?\n",
    "print(true_positive_percentage1)\n",
    "print(true_positive_percentage2)\n",
    "print(true_positive_percentage3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b9e3ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90e723a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog    3254\n",
      "cat    1746\n",
      "Name: actual, dtype: int64\n",
      "model 1 0.8074\n",
      "model 2 0.6304\n",
      "model 3 0.5096\n",
      "model 4 0.7426\n",
      "baseline 0.6508\n"
     ]
    }
   ],
   "source": [
    "# You are working as a data scientist for Gives You Paws â„¢, \n",
    "# a subscription based service that shows you cute pictures of \n",
    "# dogs or cats (or both for an additional fee).\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# At Gives You Paws, anyone can upload pictures of their cats or dogs. \n",
    "# The photos are then put through a two step process. \n",
    "# First an automated algorithm tags pictures as either a cat or a dog (Phase I). \n",
    "# Next, the photos that have been initially identified are put through another \n",
    "# round of review, possibly with some human oversight, before being presented to \n",
    "# the users (Phase II).\n",
    "df = pd.read_csv('gives_you_paws.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Several models have already been developed with the data, and you can \n",
    "# find their results here.\n",
    "\n",
    "# Given this dataset, use pandas to create a baseline model \n",
    "# (i.e. a model that just predicts the most common class) \n",
    "# and answer the following questions:\n",
    "\n",
    "\n",
    "\n",
    "# df['baseline_prediction'] = \"cat\"\n",
    "\n",
    "# baseline = (df.actual == df.baseline_prediction).mean()\n",
    "\n",
    "\n",
    "# subset = df[df.actual == 'cat']\n",
    "# model1 = (subset.actual == subset.model1).mean()\n",
    "# model2 = (subset.actual == subset.model2).mean()\n",
    "# model3 = (subset.actual == subset.model3).mean()\n",
    "\n",
    "# print(model1)\n",
    "# print(model2)\n",
    "# print(model3)\n",
    "# pd.crosstab(df.model1, df.actual)\n",
    "\n",
    "\n",
    "\n",
    "# In terms of accuracy, how do the various models compare to the baseline model? \n",
    "\n",
    "# Are any of the models better than the baseline?\n",
    "df['baseline'] = 'dog'\n",
    "model1_actual = (df.actual == df.model1).mean()\n",
    "model2_actual = (df.actual == df.model2).mean()\n",
    "model3_actual = (df.actual == df.model3).mean()\n",
    "model4_actual = (df.actual == df.model4).mean()\n",
    "baseline = (df.actual == df.baseline).mean()\n",
    "print(df.actual.value_counts())\n",
    "\n",
    "the_list = [model1_actual, model2_actual, model3_actual, model4_actual]\n",
    "count = 1\n",
    "for i in range(4):\n",
    "    print('model ' + str(count), the_list[count - 1])\n",
    "    count +=1\n",
    "    \n",
    "print('baseline', baseline)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9d4c8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 0.803318992009834\n",
      "model 2 0.49078057775046097\n",
      "model 3 0.5086047940995697\n",
      "model 4 0.9557467732022127\n",
      "baseline 1.0\n",
      "dog    3254\n",
      "cat    1746\n",
      "Name: actual, dtype: int64\n",
      "     actual model1 model2 model3 model4 baseline\n",
      "1       dog    dog    cat    cat    dog      dog\n",
      "2       dog    cat    cat    cat    dog      dog\n",
      "3       dog    dog    dog    cat    dog      dog\n",
      "5       dog    dog    dog    dog    dog      dog\n",
      "8       dog    dog    cat    dog    dog      dog\n",
      "...     ...    ...    ...    ...    ...      ...\n",
      "4993    dog    dog    cat    dog    dog      dog\n",
      "4995    dog    dog    dog    dog    dog      dog\n",
      "4996    dog    dog    cat    cat    dog      dog\n",
      "4997    dog    cat    cat    dog    dog      dog\n",
      "4999    dog    dog    dog    dog    dog      dog\n",
      "\n",
      "[3254 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Suppose you are working on a team that solely deals with dog pictures. \n",
    "# Which of these models would you recommend?\n",
    "\n",
    "\n",
    "subset = df[df.actual == 'dog']\n",
    "model_1_dog_predicts = (subset.actual == subset.model1).mean()\n",
    "model_2_dog_predicts = (subset.actual == subset.model2).mean()\n",
    "model_3_dog_predicts = (subset.actual == subset.model3).mean()\n",
    "model_4_dog_predicts = (subset.actual == subset.model4).mean()\n",
    "baseline = (subset.actual == subset.baseline).mean()\n",
    "\n",
    "the_list = [model_1_dog_predicts, model_2_dog_predicts, model_3_dog_predicts, model_4_dog_predicts]\n",
    "count = 1\n",
    "for i in range(4):\n",
    "    print('model ' + str(count), the_list[i])\n",
    "    count +=1\n",
    "print('baseline', baseline)\n",
    "print(df.actual.value_counts())\n",
    "print(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66c799d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 0.6897721764420747\n",
      "model 2 0.4841220423412204\n",
      "model 3 0.358346709470305\n",
      "model 4 0.8072289156626506\n"
     ]
    }
   ],
   "source": [
    "# Suppose you are working on a team that solely deals with cat pictures. \n",
    "# Which of these models would you recommend?\n",
    "\n",
    "# subset = df[df.actual == 'cat']\n",
    "\n",
    "# model_1_cats = (subset.actual == subset.model1).mean()\n",
    "# model_2_cats = (subset.actual == subset.model2).mean()\n",
    "# model_3_cats = (subset.actual == subset.model3).mean()\n",
    "# model_4_cats = (subset.actual == subset.model4).mean()\n",
    "\n",
    "# count = 1\n",
    "# the_list = [model_1_cats,model_2_cats,model_3_cats,model_4_cats]\n",
    "# for i in range(4):\n",
    "#     print('model ' + str(count), (the_list[i] * 100).round(2))\n",
    "#     count +=1\n",
    "\n",
    "subset1 = df[df.model1 == 'cat']\n",
    "subset2 = df[df.model2 == 'cat']\n",
    "subset3 = df[df.model3 == 'cat']\n",
    "subset4 = df[df.model4 == 'cat']\n",
    "\n",
    "model_1_accuracy_cat = (subset1.actual == subset1.model1).mean()\n",
    "model_2_accuracy_cat = (subset2.actual == subset2.model2).mean()\n",
    "model_3_accuracy_cat = (subset3.actual == subset3.model3).mean()\n",
    "model_4_accuracy_cat = (subset4.actual == subset4.model4).mean()\n",
    "\n",
    "the_list = [model_1_accuracy_cat, model_2_accuracy_cat,model_3_accuracy_cat,model_4_accuracy_cat]\n",
    "count = 1\n",
    "for i in range(4):\n",
    "    print('model ' + str(count), the_list[i])\n",
    "    count +=1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0310065f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.689772</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.816408</td>\n",
       "      <td>1423.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.689772</td>\n",
       "      <td>0.689772</td>\n",
       "      <td>0.689772</td>\n",
       "      <td>0.689772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.344886</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.408204</td>\n",
       "      <td>2063.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.475786</td>\n",
       "      <td>0.689772</td>\n",
       "      <td>0.563136</td>\n",
       "      <td>2063.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "cat            0.689772  1.000000  0.816408  1423.000000\n",
       "dog            0.000000  0.000000  0.000000   640.000000\n",
       "accuracy       0.689772  0.689772  0.689772     0.689772\n",
       "macro avg      0.344886  0.500000  0.408204  2063.000000\n",
       "weighted avg   0.475786  0.689772  0.563136  2063.000000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Follow the links below to read the documentation \n",
    "# about each function, then apply those functions \n",
    "# to the data from the previous problem.\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.DataFrame(classification_report(subset1.actual, subset1.model1,\n",
    "                                  labels = ['cat', 'dog'],\n",
    "                                  output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bcdd607e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.484122</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.652402</td>\n",
       "      <td>1555.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1657.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.484122</td>\n",
       "      <td>0.484122</td>\n",
       "      <td>0.484122</td>\n",
       "      <td>0.484122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.242061</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.326201</td>\n",
       "      <td>3212.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.234374</td>\n",
       "      <td>0.484122</td>\n",
       "      <td>0.315842</td>\n",
       "      <td>3212.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "cat            0.484122  1.000000  0.652402  1555.000000\n",
       "dog            0.000000  0.000000  0.000000  1657.000000\n",
       "accuracy       0.484122  0.484122  0.484122     0.484122\n",
       "macro avg      0.242061  0.500000  0.326201  3212.000000\n",
       "weighted avg   0.234374  0.484122  0.315842  3212.000000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(classification_report(subset2.actual, subset2.model2,\n",
    "                                  labels = ['cat', 'dog'],\n",
    "                                  output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "57266361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.358347</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.527622</td>\n",
       "      <td>893.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.358347</td>\n",
       "      <td>0.358347</td>\n",
       "      <td>0.358347</td>\n",
       "      <td>0.358347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.179173</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.263811</td>\n",
       "      <td>2492.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.128412</td>\n",
       "      <td>0.358347</td>\n",
       "      <td>0.189072</td>\n",
       "      <td>2492.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score      support\n",
       "cat            0.358347  1.000000  0.527622   893.000000\n",
       "dog            0.000000  0.000000  0.000000  1599.000000\n",
       "accuracy       0.358347  0.358347  0.358347     0.358347\n",
       "macro avg      0.179173  0.500000  0.263811  2492.000000\n",
       "weighted avg   0.128412  0.358347  0.189072  2492.000000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(classification_report(subset3.actual, subset3.model3,\n",
    "                                  labels = ['cat', 'dog'],\n",
    "                                  output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e8309e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.807229</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>603.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>144.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.807229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.403614</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.446667</td>\n",
       "      <td>747.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.651619</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.721124</td>\n",
       "      <td>747.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score     support\n",
       "cat            0.807229  1.000000  0.893333  603.000000\n",
       "dog            0.000000  0.000000  0.000000  144.000000\n",
       "accuracy       0.807229  0.807229  0.807229    0.807229\n",
       "macro avg      0.403614  0.500000  0.446667  747.000000\n",
       "weighted avg   0.651619  0.807229  0.721124  747.000000"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(classification_report(subset4.actual, subset4.model4,\n",
    "                                  labels = ['cat', 'dog'],\n",
    "                                  output_dict=True)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1b66bd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['model1', 'model2', 'model3', 'model4', 'baseline'], dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "subset1.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "106b9c68",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and multiclass-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactual\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset1\u001b[49m\u001b[43m[\u001b[49m\u001b[43msubset1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdog\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1769\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprecision_score\u001b[39m(\n\u001b[1;32m   1641\u001b[0m     y_true,\n\u001b[1;32m   1642\u001b[0m     y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1648\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1649\u001b[0m ):\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m \n\u001b[1;32m   1652\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1769\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1779\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1556\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m beta \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1555\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta should be >=0 in the F-beta score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1556\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1357\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[0;32m-> 1357\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     90\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     95\u001b[0m             type_true, type_pred\n\u001b[1;32m     96\u001b[0m         )\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    100\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and multiclass-multioutput targets"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "precision_score(subset1.actual, subset1[subset1.columns[1:]], pos_label='dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "788bf5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(predictions, positive='dog'):\n",
    "    \"\"\"\n",
    "    This function will: \n",
    "    - take in a model prediction \n",
    "    - defaulted pos class is dog\n",
    "    - output the precision score\n",
    "    \"\"\"\n",
    "    return precision_score(subset1.actual, predictions, pos_label=positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "95b54638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(predictions, positive='dog'):\n",
    "    \"\"\"\n",
    "    This function will: \n",
    "    - take in a model prediction \n",
    "    - defaulted pos class is dog\n",
    "    - output the recall score\n",
    "    \"\"\"\n",
    "    return recall_score(subset1.actual, predictions, pos_label=positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3fd0d6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>0.510938</td>\n",
       "      <td>0.670082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>0.520312</td>\n",
       "      <td>0.324878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>0.967187</td>\n",
       "      <td>0.400907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.310228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            recall  precision\n",
       "model1    0.000000   0.000000\n",
       "model2    0.510938   0.670082\n",
       "model3    0.520312   0.324878\n",
       "model4    0.967187   0.400907\n",
       "baseline  1.000000   0.310228"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([\n",
    "    subset1.loc[:, 'model1':'baseline'].apply(calculate_recall).rename('recall'),\n",
    "    subset1.loc[:, 'model1':'baseline'].apply(calculate_precision).rename('precision'),\n",
    "], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
